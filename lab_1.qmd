---
title: "Lab 1: Early Phase Tasks & Tools"
format:
  html:
    self-contained: true
editor: source
knitr:
  opts_chunk: 
    message: false
    warning: false
---

# Overview

In the early stages of an outbreak response, the questions revolve around trying to understand if something is happening and, if so, how concerning is that happening?

In this phase, the calculations are computational simple. This reflects sparse data and tolerance for simplifying assumptions that tend to be risk-averse. However, there is often relatively complex work in this phase surrounding collecting and curating data into useful streams for analysis.

In this session, we introduce several tools for working with common data you might see early in outbreak. You will practice with these tools to create a clean data stream, which you then use in analyses tailored to assessing overall threat from a pathogen, like epidemic growth rate and reproductive number, case fatality risks, and other measures.

Throughout the session, when you see `{somepackage}` that indicates an `R` package. In code blocks, we will also tend to use the syntax `somepackage::somefunction()` when using a function from a package rather than standard `R`. While this syntax is unnecessary if you have already loaded a package with `library(somepackage)`, for learning purposes it highlights where functions come from.

Finally, for these labs we assume that you are working within the project created by cloning the `SISMID_course` repository.

# Basics of Data Sources

Working in `R`, the initial step is typically loading the data into the programming environment from either a local source (like a file on your computer) or external source (like a database). Outbreak-relevant data can be stored in diverse formats, from general purpose relational databases, health information systems like [REDCap](https://www.project-redcap.org/) and [DHIS2](https://dhis2.org/), but also rushed spreadsheets with manual entry. The more systematic approaches tend to include processes and procedures which tend to improve data quality, though also entail advanced planning and effort and thus cost. Outbreak and early epidemic data tend towards spreadsheet style, as they represent unanticipated events; even in settings with well-developed health surveillance systems, an outbreak typically represents novel characteristics and metrics not contemplated when setting up that system.

## Import & Cleaning Tools

First, we will use `{rio}` to read data and `{here}` to assure locating files within the file system. We also adopt using the native `R` pipe `|>`; if you are already familiar with `%>%`, native pipe works almost identically. If this operator is new to you: it works by redirecting (or "piping") output of one operation into another. We are going to use the convenient transformation operations from `{dplyr}`. Lastly, we will use `{here}` to make sure that we can reference files in a way that is independent of the computer or operating system you are using.

Several packages are available for importing outbreak data stored in individual files into `R`. These include [`{rio}`](https://gesistsa.github.io/rio/), [`{readr}`](https://readr.tidyverse.org/) from the `{tidyverse}`, [`{io}`](https://bitbucket.org/djhshih/io/src/master/), [`{ImportExport}`](https://cran.r-project.org/web/packages/ImportExport/index.html), and [`{data.table}`](https://rdatatable.gitlab.io/data.table/). Together, these packages offer methods to read single or multiple files in a wide range of formats.

```{r,eval=TRUE,message=FALSE,warning=FALSE}
# Load packages
library(dplyr) # for {dplyr} functions
library(rio) # for importing data
library(here) # for easy file referencing
```

## Reading from Files

The below example shows how to import a `csv` file into `R` environment using `{rio}` package. We use the `{here}` package to tell R to look for the file in the `data/` folder of your project, and `as_tibble()` to convert into a tidier format for subsequent analysis in R.

```{r,eval=TRUE,echo=TRUE}
# read data
# e.g., the path to our file is data/ebola_cases.csv then:
ebola_confirmed <- here::here("data", "ebola_cases.csv") |>
  rio::import() |>
  dplyr::as_tibble()
ebola_confirmed
```

🧠 **Question**: what other formats can you import with `{rio}`? Check your answer with the `?rio::import` command.

Quick aside on the `{here}` package: `{here}` is designed to (1) simplify file referencing (2) in R projects. It does so by providing a reliable way to construct file paths relative to a project root. This makes switching between environments, that is: different computers, different operating systems. Critically: `{here}` relies on a being able to identify a project root; in this tutorial, that is possible because there is an `.Rproj` file (if you cloned the repository per the instructions), but it supports other anchor files as well - have a look at `?here::here` to see the others. One key caveat: `here::here()` is intended to be used in interactive sessions, such as in RStudio, and may not work as expected in non-interactive contexts.

🧠 **Question**: what are some key contexts where `{here}` might *not* work?

Aside: The `{here}` package can help manage reproducibility to your work. If you are interested in a tutorial on a wide variety of reproducibility practices and tools in R, consider [this tutorial from Epiverse](https://epiverse-trace.github.io/research-compendium/aio.html).

Now let's read in some other data - there's zipped data for the Marburg outbreak in same place as the Ebola data. Modify the code below to read in the Marburg data.

```{r,eval=TRUE,echo=TRUE}
# read data
# e.g., the path to our file is data/Marburg.zip then:
marburg_data <- "..." |>
  dplyr::as_tibble()
marburg_data
```

Hint: you can use `{here}` and `{rio}` again; check the [full list of `{rio}` supported file formats](https://gesistsa.github.io/rio/#supported-file-formats) on the package website (which also includes advice on expanding to additional formats).

## Reading from Databases

In addition to reading data from files, we can also read data directly from databases. This is particularly useful when dealing with large datasets or when data is stored in a relational database management system (RDBMS). We get the data using "queries" - queries tell the database what to get and how to return it and even execute some calculations, which can help optimize performance and reduce memory usage. External RDBMS also have the advantage that multiple users can access, store and analyse parts of the dataset simultaneously, without having to transfer individual files, which would make it very difficult to track which version is up-to-date.

The [`{DBI}`](https://dbi.r-dbi.org/) package defines a uniform interface for interacting with database management systems (DBMS) across different back-ends or servers. For this tutorial, we will use the [`{RSQLite}`](https://r-dbi.github.io/RSQLite/) package to connect to an SQLite database. SQLite is a lightweight, file-based database system that is easy to set up and use for small to medium-sized datasets. The code below, however, can be adapted to connect to other types of databases (e.g., MySQL, PostgreSQL) by changing the driver and connection parameters.

The following code chunks demonstrate in four steps how to create a temporary SQLite database in memory, store the `ebola_confirmed` as a table on it, and subsequently read it:

```{r,warning=FALSE,message=FALSE}
library(DBI)
library(RSQLite)

db_connection <- DBI::dbConnect(
  drv = RSQLite::SQLite(),
  dbname = ":memory:" # Creates a temporary database in memory
)
```

A connection to an local database on disk would look like:

``` r
db_connection <- DBI::dbConnect(
  drv = RSQLite::SQLite(),
  dbname = here("path", "to", "database.sqlite") # connect to one on disk
)
```

... or to a remote database:

``` r
db_connection <- DBI::dbConnect(
  drv = RSQLite::SQLite(), 
  host = "database.epiversetrace.com",
  user = "juanito",
  password = epiversetrace::askForPassword("Database password")
)
```

Then, we can write the `ebola_confirmed` into a table named `cases` within the database using the `DBI::dbWriteTable()` function.

```{r,warning=FALSE,message=FALSE}
# Store the 'ebola_confirmed' dataframe as a table named 'cases'
# in the SQLite database defined by `db_connection`
DBI::dbWriteTable(
  conn = db_connection,
  name = "cases",
  value = ebola_confirmed
)
```

Aside: A database typically has multiple tables, usually each corresponding a distinct `entity` (e.g., patients, care units, jobs). Those tables can then be related (this is where the *relational* comes from in RDBMS) by a common IDs, which are referred to as *primary keys*, and *foreign keys*. We only work with one table in this example, but for an extended example of working with multiple tables, you might consider [this tutorial](https://ecorepsci.github.io/reproducible-science/rsqlite.html) or [this one](https://datacarpentry.github.io/sql-ecology-lesson/aio.html). For the `tidyverse` approach (which can somewhat manage key-join relationships for you) you can refer to the [dbplyr vignette](https://dbplyr.tidyverse.org/articles/dbplyr.html).

Subsequently, we read the data from the `cases` table using `dplyr::tbl()`.

```{r}
# Read one table from the database
mytable_db <- dplyr::tbl(src = db_connection, "cases")
```

When working with databases, `{dplyr}` will translate operations (e.g. `filter()`) into SQL queries. The database is *not* queryed (and thus results loaded into memory) until you take an action that would actually need the results (e.g. printing the results or explicitly calling `dplyr::collect()`). This allows you to work with large datasets without loading them all into memory at once.

```{r}
# Show the SQL queries translated
mytable_db |>
  dplyr::filter(confirm > 50) |>
  dplyr::arrange(desc(confirm)) |>
  dplyr::show_query()
```

So now we use `dplyr::collect()` to force computation of a database query and extract the output to your local computer.

```{r}
# Pull all data down to a local tibble
extracted_data <- mytable_db |>
  dplyr::filter(confirm > 50) |>
  dplyr::arrange(desc(confirm)) |>
  dplyr::collect()
extracted_data
```

The `extracted_data` object is there filtered (and otherwise transformed) data from the database. In a real context, this approach could substantially reduce how much data are loaded in memory for work in `R`.

Now that we have done all our operations, we should close the database connection with `dbDisconnect()`. In the case of a `:memory:` database, this effectively destroys the object. For on-disk or remote databases, this cleans up all pending queries and critically potentially releases read/write locks.

```{r,warning=FALSE,message=FALSE}
# Close the database connection
DBI::dbDisconnect(conn = db_connection)
```

## Reading from Health Information System (HIS) APIs

Health related data are also increasingly stored in specialized HIS APIs like **Fingertips**, **GoData**, **REDCap**, and **DHIS2**. We do not cover these more bespoke systems in this tutorial, but if you do use them in your work, you should consider looking at the [`{readepi}`](https://epiverse-trace.github.io/readepi/) package which is designed to work with HIS APIs.

# Working with Outbreak Data

## Seeing Problems

After curating data from multiple sources and in multiple formats, the next step is to ensure those data are clean, standardized, and validated. If you skip this step, you may not be analyzing what you think you are. Similarly, anyone attempting to reproduce your work may be left to guesswork about assumptions, and unlikely to obtain the same results.

We are going to use the [`{cleanepi}`](https://epiverse-trace.github.io/cleanepi/) package on a simulated dataset of Ebola cases. As before, we will use `{here}`, `{rio}`, and `{dplyr}` to *load* the data:

```{r,eval=TRUE,echo=TRUE}
raw_ebola_data <- here::here("data", "simulated_ebola.csv") |>
  rio::import() |>
  dplyr::as_tibble()
raw_ebola_data
```

🧠 **Question**: Compared to the earlier data we used to demonstrate the basics of reading data, what do you notice?

Hopefully you identified some problems in these general categories:

-   *Codification*: Do all the entries in a column have the same data *type*? At a most basic level, typically integers versus reals vs character strings. Some types have higher level: a specific format to strings, words / letters from a particular set only, or strictly positive integers.
-   *Inconsistency*: If columns are somehow related, are the values in a row consistent with that relationship? If rows are somehow related, are the values within a column consistent with that relationship?
-   *Non-plausible values*: Where values conform to the appropriate type, are they reasonable in the context of the dataset?
-   *Missingness*: How do the data capture missing values? Does an `NA` actually define missing and/or is there some other value?
-   *Duplicates*: Are all observations unique?
-   *Irrelevance*: Is a column empty? Or perhaps constant-valued?

We can also use the `{cleanepi}` package to do a more-than-by-eye quick inspection of the dataset to identify potential issues with the `scan_data()` function:

```{r}
library(cleanepi)
cleanepi::scan_data(raw_ebola_data)
```

The results provide an overview of the content of every column, including column names, and the percent of some data types per column. You can see that the column names in the dataset are descriptive but lack consistency, as some they are composed of multiple words separated by white spaces. Additionally, some columns contain more than one data type, and there are missing values in others.

## Fixing Problems

### Standardizing Column Names

For this example dataset, standardizing column names typically involves removing spaces and connecting different words with “\_”. This practice helps maintain consistency and readability in the dataset. However, the function used for standardizing column names offers more options, for example you can exclude columns from standardization (use `?cleanepi::standardize_column_names` for more details).

```{r}
sim_ebola_data <- cleanepi::standardize_column_names(raw_ebola_data)
names(sim_ebola_data)
```

### Dealing with Duplication and Irrelevance

Raw data may contain irregularities such as **duplicated** rows, **empty** rows and columns, or **constant** columns (where all entries have the same value). Functions from `{cleanepi}` like `remove_duplicates()` and `remove_constants()` can remove such irregularities as demonstrated in the below code chunk.

```{r}
# Remove constants
sim_ebola_data <- cleanepi::remove_constants(sim_ebola_data)
sim_ebola_data
```

```{r}
# Remove duplicates
sim_ebola_data <- cleanepi::remove_duplicates(sim_ebola_data)
sim_ebola_data
```

🧠 **Question**: What changed? How can you tell?

You can get the number and location of the duplicated rows that where found. Run `cleanepi::print_report()`, wait for the report to open in your browser, and find the "Duplicates" tab.

```{r,echo=FALSE,eval=FALSE}
# Print a report; note, this is not run when compiling the qmd
library(reactable)
cleanepi::print_report(sim_ebola_data)
```

### Replacing Missing Values

In addition to the irregularities, raw data may contain missing values, and these may be encoded by different strings (e.g. `"NA"`, `""`, `character(0)`). To ensure robust analysis, it is a good practice to replace all missing values by `NA` in the entire dataset. Below is a code snippet demonstrating how you can achieve this in `{cleanepi}` for missing entries represented by an empty string `"`:

```{r}
sim_ebola_data <- cleanepi::replace_missing_values(
  sim_ebola_data,
  na_strings = ""
)
sim_ebola_data
```

Note: if you `print_report()` again, you can now see the `NA`-related changes; the function will not automatically update the report that appears in your browser, but can be re-run with each step.

### Validating Subject IDs

Each entry in this dataset represents a subject (e.g. a disease case or study participant) and should be distinguishable by a specific column formatted in a particular way, such as falling within a specified range, containing certain prefixes and/or suffixes, containing a specific number of characters. The `{cleanepi}` package offers the function `check_subject_ids()` designed precisely for this task as shown in the below code chunk. This function validates whether they are unique and meet the required criteria.

```{r}
sim_ebola_data <- cleanepi::check_subject_ids(
  sim_ebola_data,
  target_columns = "case_id",
  range = c(0, 15000)
)
```

Notice that this call emits information about for example duplicate subject ids. Once again, we can refer to `print_report()` to see where there may be issues - focus on "Unexpected subject ids" tab to identify what IDs require attention. We are not going to do more with cleaning this issue, but you can get further information at the associated [`{cleanepi}` documentation](https://epiverse-trace.github.io/cleanepi/reference/check_subject_ids.html).

### Standardizing Dates

An epidemic dataset typically contains date columns for different events, such as the date of infection, date of symptoms onset, etc. These dates can come in different date formats, both within and across columns, but they likely need to standardized so that subsequent analysis can compare or calculate both within and across columns.

The `{cleanepi}` package provides functionality for converting date columns of epidemic datasets into ISO format, ensuring consistency across the different date columns. Here's how you can use it on our simulated dataset:

```{r}
sim_ebola_data <- cleanepi::standardize_dates(
  sim_ebola_data,
  target_columns = c(
    "date_onset",
    "date_sample"
  )
)
sim_ebola_data
```

This function converts the values in the target columns, but if `target_columns = NULL` (the default) will attempt to automatically identify date columns and convert them into the **Ymd** format.

### Converting to Numeric Values

In real data, sometimes numerical values are recorded with a mixture of character and numeral values. For calculations, these need to be standardized to numeric values (e.g. `"seven"` to `7`). In `{cleanepi}` the function `convert_to_numeric()` does such conversion as illustrated in the below code chunk.

```{r}
sim_ebola_data <- cleanepi::convert_to_numeric(sim_ebola_data,
  target_columns = "age"
)
sim_ebola_data
```

At this time, this operation does *not* result in an updated `print_report()` output.

🧠 **Question**: What steps could you take to check which values were changed?

## Epidemiology Related Operations

In addition to general purpose cleaning, the `{cleanepi}` package also has functions tailored to outbreak and epidemic data. This section covers some of these specialized tasks.

### Checking sequence of Dated-Events

Ensuring the correct order and sequence of dated events is crucial in epidemiological data analysis, especially when analyzing infectious diseases where the timing of events like symptom onset and sample collection is essential. The `{cleanepi}` package provides a helpful function called `check_date_sequence()` precisely for this purpose.

Here's an example of a code chunk demonstrating the usage of the function `check_date_sequence()` in the first 100 records of our simulated Ebola dataset; note that this kind of analysis *only* works on standardized date data, so you cannot skip ahead to this step.

```{r, warning=FALSE, results = 'hide'}
cleanepi::check_date_sequence(
  sim_ebola_data[1:100, ],
  target_columns = c("date_onset", "date_sample")
)
```

This functionality is crucial for ensuring data integrity and accuracy in epidemiological analyses, as it helps identify any inconsistencies or errors in the chronological order of events, allowing you to address them appropriately. We just looked at a sample here, but if you want to see a full report of issues, you can:

```{r, eval=FALSE}
sim_ebola_data <- cleanepi::check_date_sequence(
  sim_ebola_data,
  target_columns = c("date_onset", "date_sample")
)
cleanepi::print_report(sim_ebola_data)
```

### Dictionary-based Substitution

For columns with a code-based entry, such as the “gender” column in our simulated Ebola dataset, it is typically necessary to have specific values or factors when doing, for example, regressions. However, it's also common for unexpected or erroneous values to appear in these columns, which need to be replaced with appropriate values. The `{cleanepi}` package offers support for dictionary-based substitution, a method that allows you to replace values in specific columns based on mappings defined in a dictionary. This approach ensures consistency and accuracy in data cleaning.

For this example we will use a demo dictionary from `{cleanepi}`, which provides mappings for a “gender” column.

```{r}
test_dict <- base::readRDS(
  system.file("extdata", "test_dict.RDS", package = "cleanepi")
) |>
  dplyr::as_tibble() # for a simple data frame output
test_dict
```

Now, we can use this dictionary to standardize values of the the “gender” column according to these predefined categories:

```{r}
sim_ebola_data <- cleanepi::clean_using_dictionary(
  sim_ebola_data,
  dictionary = test_dict
)
sim_ebola_data
```

This approach captures the data cleaning process *and* can make the dictionary available for other researchers to use and inspect.

Note that, when the column in the dataset contains values that are not in the dictionary, the function `cleanepi::clean_using_dictionary()` will raise an error.

You can start a custom dictionary with a data frame inside or outside R. You can use the function `cleanepi::add_to_dictionary()` to include new elements in the dictionary. For example:

```{r}
new_dictionary <- tibble::tibble(
  options = "0",
  values = "female",
  grp = "sex",
  orders = 1L
) |> cleanepi::add_to_dictionary(
  option = "1",
  value = "male",
  grp = "sex",
  order = NULL
)

new_dictionary
```

You can read more details in the section about "Dictionary-based data substituting" in the package ["Get started" vignette](https://epiverse-trace.github.io/cleanepi/articles/cleanepi.html#dictionary-based-data-substituting).

### Calculating Time Span Between Different Date Events

In epidemiological data analysis, it is also useful to track and analyze time-dependent events, such as the progression of a disease outbreak (i.e., the time difference between today and the first case reported) or the duration between sample collection and analysis (i.e., the time difference between today and the sample collection). The most common example is to calculate the age of all the subjects given their date of birth (i.e., the time difference between today and the date of birth).

The `{cleanepi}` package offers a convenient function for calculating the time elapsed between two dated events at different time scales. For example, the below code snippet utilizes the function `cleanepi::timespan()` to compute the time elapsed since the date of sample for the case identified until the 3rd of January 2025 (`"2025-01-03"`).

```{r}
sim_ebola_data <- cleanepi::timespan(
  sim_ebola_data,
  target_column = "date_sample",
  end_date = lubridate::ymd("2025-01-03"),
  span_unit = "years",
  span_column_name = "years_since_collection",
  span_remainder_unit = "months"
)

# reviewing the computed times:
sim_ebola_data |>
  dplyr::select(case_id, date_sample, years_since_collection, remainder_months)
```

After executing the function `cleanepi::timespan()`, two new columns named `years_since_collection` and `remainder_months` are added to the **sim_ebola_data** dataset, containing the calculated time elapsed since the date of sample collection for each case, measured in years, and the remaining time measured in months.

## Multiple Operations at Once

As you are developing the cleaning regimen, you will typically have a step at a time (as shown in the above series of chunks). However, if you do not consolidate these steps as you work through them, you run the risk errors - running steps out of order, forgetting a step, etc. To consolidate as you work, you can combine multiple data cleaning tasks via the pipe operator in `|>`, as shown in the below code snippet.

```{r,warning = FALSE, message = FALSE}
# Perfom the cleaning operations using the pipe (|>) operator
cleaned_data <- raw_ebola_data |>
  cleanepi::standardize_column_names() |>
  cleanepi::remove_constants() |>
  cleanepi::remove_duplicates() |>
  cleanepi::replace_missing_values(na_strings = "") |>
  cleanepi::check_subject_ids(
    target_columns = "case_id",
    range = c(1, 15000)
  ) |>
  cleanepi::standardize_dates(
    target_columns = c("date_onset", "date_sample")
  ) |>
  cleanepi::convert_to_numeric(target_columns = "age") |>
  cleanepi::check_date_sequence(
    target_columns = c("date_onset", "date_sample")
  ) |>
  cleanepi::clean_using_dictionary(dictionary = test_dict) |>
  cleanepi::timespan(
    target_column = "date_sample",
    end_date = lubridate::ymd("2025-01-03"),
    span_unit = "years",
    span_column_name = "years_since_collection",
    span_remainder_unit = "months"
  )
```

Then you can record the result:

```{r,echo=FALSE,eval=TRUE}
cleaned_data |>
  readr::write_csv(file = file.path("data", "cleaned_data.csv"))
```

The `{cleanepi}` package also offers wrapper function called `clean_data()`, which allows you to perform multiple operations at once. The `clean_data()` function applies the supplied series of data cleaning operations to the input dataset; if working from an externally defined set of operations that you can read in, this can be faster to implement than manually constructing all the function calls.

# Validating Data

Once you have read and cleaned the case data, it's essential to establish an additional foundation layer to ensure the integrity and reliability of subsequent analyses. Otherwise you might find that your analysis suddenly stops working when specific variables appear or disappear, or their underlying data types (like `<date>` or `<chr>`) change. Specifically, this additional layer involves: 1) verifying the presence and correct data type of certain columns within your dataset, a process commonly referred to as "tagging"; 2) implementing measures to check that these tagged columns are not inadvertently deleted during further data processing steps, known as "validation".

This element focuses tagging and validate outbreak data using the [`{linelist}`](https://epiverse-trace.github.io/linelist/) package. We will use previously generated data, but imagine you are in an emergency response situation, where you have automated an analysis to read data directly from the online source and then generate reports. Suddenly, however, that source changed a key variable you have been using!

How could you detect if the data input is **still valid** to replicate the analysis code you wrote the day before? That is the focus of this section.

```{r,eval=TRUE,message=FALSE,warning=FALSE}
# Load packages
library(linelist) # for tagging and validating
```

```{r}
cleaned_data <- rio::import(
  here::here("data", "cleaned_data.csv")
) |>
  dplyr::as_tibble() # for a simple data frame output
cleaned_data
```

## Creating a Linelist and Tagging Elements

Once the data is loaded and cleaned, we convert the cleaned case data into a `linelist` object using `{linelist}` package, as in the below code chunk.

```{r}
library(linelist)
# Create a linelist object from cleaned data
linelist_data <- linelist::make_linelist(
  x = cleaned_data,         # Input data
  id = "case_id",            # Column for unique case identifiers
  date_onset = "date_onset", # Column for date of symptom onset
  gender = "gender"          # Column for gender
)

# Display the resulting linelist object
linelist_data
```

The `{linelist}` package supplies tags for common epidemiological variables and a set of appropriate data types for each. You can view the list of available tags by the variable name and their acceptable data types for each using `linelist::tags_types()`.

## Validation with `{linelist}`

To ensure that all tagged variables are standardized and have the correct data types, use the `linelist::validate_linelist()`, as shown in the example below:

```{r}
linelist::validate_linelist(linelist_data)
```

Now we will practice validating some tagged variables by simulating a situation in an ongoing outbreak. Imagine you wake up to discover that the data stream you rely on has a new set of entries (which is normal) and one variable has a changed data type (which is not normal).

For example, let's assume the variable `age` changed from a double (`<dbl>`) variable to character (`<chr>`). To simulate that, we can ...

```{r, eval=FALSE}
unclean_data <- cleaned_data |>
  # simulate a change of data type in one variable
  dplyr::mutate(age = as.character(age))

unclean_ll <- unclean_data |> linelist::make_linelist(
  age = "age"
)

unclean_ll |> linelist::validate_linelist()
```

Can imagine other situations and what errors they might report?

-   `date_onset` changes from a `<date>` variable to character (`<chr>`),
-   `gender` changes from a character (`<chr>`) variable to integer (`<int>`).

Feel free to repeat the above validation test. Does the `Error` message propose to us the solution?

Now imagine losing variables; you can simulate this scenario as:

```{r, eval=FALSE}
cleaned_data |>
  # simulate a change of data type in one variable
  select(-age) |>
  # tag one variable
  linelist::make_linelist(
    age = "age"
  )
```

What error is produced now?

## Safeguarding

Safeguarding is implicitly built into the linelist objects. If you try to drop any of the tagged columns, you will receive an error or warning message, as shown in the example below.

```{r, warning=TRUE}
new_df <- linelist_data |>
  dplyr::select(case_id, gender)
```

This `Warning` message above is the default output option when we lose tags in a `linelist` object. However, it can be changed to an `Error` message using `linelist::lost_tags_action()`.

Aside: The `linelist` object resembles a `data.frame` but with richer features. Other packages that are `linelist`-aware can leverage these features. For example, you can extract a data frame of only the tagged columns using the `linelist::tags_df()` function, as shown below:

```{r, warning=FALSE}
linelist::tags_df(linelist_data)
```

This provides a convenient way to ensure downstream analysis only uses tagged columns in downstream analysis.

## When Should I Use `{linelist}`?

Data analysis during an outbreak response demands a different set of "data safeguards" when compared to usual research situations. Notably, there is a continuous stream of data, and likely a changing observational standard.

`{linelist}` is more appropriate for this type of ongoing or long-lasting analysis. You can learn more in the "Get started" vignette section about [When you should consider using `{linelist}`?](https://epiverse-trace.github.io/linelist/articles/linelist.html#should-i-use-linelist).

# Visualizing Epidemic Data

Before investing effort in formal modeling, exploratory data analysis can suggest relationships between variables and summarize characteristics, often by means of data visualization. This preliminary work can help ensure subsequent modeling work is more efficiently targetted.

This exploratory analysis often focuses on *who-what-when-where* (formal modeling then deals with *why-how*). Basically, it is useful to identify *who* (gender, age, etc) experiences *what* outcomes (confirmed cases, hospitalizations, deaths, etc) and *when* and *where* these happen (and if those outcomes change over time and across the landscape).

In this section, we are going to use `{simulist}` to simulate some outbreak data, `{incidence2}` to aggregate that data according to specific characteristics, and visualize the resulting epidemic curves over time with help from `{tracetheme}` for figure formatting.

```{r,eval=TRUE,message=FALSE,warning=FALSE}
library(incidence2) # For aggregating and visualising
library(simulist) # For simulating linelist data
library(tracetheme) # For formatting figures
library(ggplot2)
```

## Synthetic outbreak data

To illustrate the practice of exploratory analysis, we need a hypothetical disease outbreak, which we will get from `{simulist}`. Its minimal configuration can generate a linelist, as shown in the below code chunk:

```{r}
# Simulate linelist data for an outbreak with size between 1000 and 1500
set.seed(1) # Set seed for reproducibility
sim_data <- simulist::sim_linelist(outbreak_size = c(1000, 1500)) |>
  dplyr::as_tibble() # for a simple data frame output

# Display the simulated dataset
sim_data
```

This linelist dataset has entries on individual-level simulated events during the outbreak.

Aside: This is the default configuration of `{simulist}`, so makes many assumptions about the pathogen. If you want to know more about `sim_linelist()` and other functionalities check the [documentation website](https://epiverse-trace.github.io/simulist/). You can also find data sets from real emergencies from the past at the [`{outbreaks}` R package](https://www.reconverse.org/outbreaks/).

## Aggregating

When thinking about population-level dynamics, we typically want to consider the number of events that occur on a particular day or week, rather than focusing on individual cases. Since real linelist data also has a variety of privacy-sensitive details, modelers may also only be permitted to have access to the event data.

Thus typically, linelist data gets converted into incidence data. The [incidence2]((https://www.reconverse.org/incidence2/articles/incidence2.html)%7B.external%20target=%22_blank%22%7D) package offers a useful function called `incidence2::incidence()` for grouping case data, usually based around dated events and/or other characteristics. The code chunk provided below demonstrates the creation of an `<incidence2>` class object from the simulated `linelist` data based on the date of onset.

```{r}
# Create an incidence object by aggregating case data based on the date of onset
daily_incidence <- incidence2::incidence(
  sim_data,
  date_index = "date_onset",
  interval = "day" # Aggregate by daily intervals
)

# View the incidence data
daily_incidence
```

With the `{incidence2}` package, you can specify the desired interval (e.g. day, week) and categorize cases by one or more factors. Below is a code snippet demonstrating weekly cases grouped by the date of onset, sex, and type of case.

```{r}
# Group incidence data by week, accounting for sex and case type
weekly_incidence <- incidence2::incidence(
  sim_data,
  date_index = "date_onset",
  interval = "week", # Aggregate by weekly intervals
  groups = c("sex", "case_type") # Group by sex and case type
)

# View the incidence data
weekly_incidence
```

## Complete Dates

When cases are grouped by different factors, it's possible that the events involving these groups may have different date ranges in the resulting `incidence2` object. The `incidence2` package provides a function called `complete_dates()` to ensure that an incidence object has the same range of dates for each group. By default, counts for a particular group will be filled with 0 for that date. In terms of *missingness*, these observations are reasonably treated as actual 0s. However, this functionality is also available as an argument within `incidence2::incidence()` by adding `complete_dates = TRUE`.

```{r}
# Create an incidence object grouped by sex, aggregating daily
daily_incidence_2 <- incidence2::incidence(
  sim_data,
  date_index = "date_onset",
  groups = "sex",
  interval = "day", # Aggregate by daily intervals
  complete_dates = TRUE # Complete missing dates in the incidence object
)
```

## Epi-curve Plots

The `incidence2` object can be visualized using the generic `plot()` function (though it will be a `ggplot2` object, for advanced users that want to use it in `ggplot2` ecosystem). The resulting graph is referred to as an epidemic curve, or epi-curve for short. The following code snippets generate epi-curves for the `daily_incidence` and `weekly_incidence` incidence objects created above.

```{r}
# Plot daily incidence data
plot(daily_incidence) +
  ggplot2::labs(
    x = "Time (in days)", # x-axis label
    y = "Dialy cases" # y-axis label
  ) +
  tracetheme::theme_trace() # Apply the custom trace theme
```

```{r}
# Plot weekly incidence data
plot(weekly_incidence) +
  ggplot2::labs(
    x = "Time (in weeks)", # x-axis label
    y = "weekly cases" # y-axis label
  ) +
  tracetheme::theme_trace() # Apply the custom trace theme
```

## Curve of Cumulative Cases

The cumulative number of cases can be calculated using the `cumulate()` function from an `incidence2` object and visualized, as in the example below.

```{r}
(incidence2::cumulate(daily_incidence) |> plot()) +
  ggplot2::labs(
    x = "Time (in days)", # x-axis label
    y = "weekly cases" # y-axis label
  ) +
  tracetheme::theme_trace() # Apply the custom trace theme
```

Note that this function preserves grouping, i.e., if the `incidence2` object contains groups, it will accumulate the cases accordingly.

## Peak Estimation

You can estimate the peak -- the time with the highest number of recorded cases-- using the `estimate_peak()` function from the `{incidence2}` package. This function employs a bootstrapping method to determine the peak time (i.e. by resampling dates with replacement, resulting in a distribution of estimated peak times).

```{r}
# Estimate the peak of the daily incidence data
peak <- incidence2::estimate_peak(
  daily_incidence,
  n = 100,         # Number of simulations for the peak estimation
  alpha = 0.05,    # Significance level for the confidence interval
  first_only = TRUE, # Return only the first peak found
  progress = FALSE  # Disable progress messages
)

# Display the estimated peak
print(peak)
```

This example demonstrates how to estimate the peak time using the `estimate_peak()` function at $95%$ confidence interval and using 100 bootstrap samples.

## Advanced Plotting with `{ggplot2}`

`{incidence2}` produces basic plots for epicurves, but additional work is required to create well-annotated graphs. However, using the `{ggplot2}` package, you can generate more sophisticated and epicurves with more flexibility in annotation. `{ggplot2}` is a comprehensive package that provides many plot types. However, we will focus on three key elements for producing epicurves: histogram plots, scaling date axes and their labels, and general plot theme annotation. The example below demonstrates how to configure these three elements for a simple `{incidence2}` object.

```{r}
# Define date breaks for the x-axis
breaks <- seq.Date(
  from = min(as.Date(daily_incidence$date_index, na.rm = TRUE)),
  to = max(as.Date(daily_incidence$date_index, na.rm = TRUE)),
  by = 20 # every 20 days
)

# Create the plot
ggplot2::ggplot(data = daily_incidence) +
  geom_histogram(
    mapping = aes(
      x = as.Date(date_index),
      y = count
    ),
    stat = "identity",
    color = "blue", # bar border color
    fill = "lightblue", # bar fill color
    width = 1 # bar width
  ) +
  theme_minimal() + # apply a minimal theme for clean visuals
  theme(
    plot.title = element_text(face = "bold",
                              hjust = 0.5), # center and bold title
    plot.subtitle = element_text(hjust = 0.5), # center subtitle
    plot.caption = element_text(face = "italic",
                                hjust = 0), # italicized caption
    axis.title = element_text(face = "bold"), # bold axis titles
    axis.text.x = element_text(angle = 45, vjust = 0.5) # rotated x-axis text
  ) +
  labs(
    x = "Date", # x-axis label
    y = "Number of cases", # y-axis label
    title = "Daily Outbreak Cases", # plot title
    subtitle = "Epidemiological Data for the Outbreak", # plot subtitle
    caption = "Data Source: Simulated Data" # plot caption
  ) +
  scale_x_date(
    breaks = breaks, # set custom breaks on the x-axis
    labels = scales::label_date_short() # shortened date labels
  )
```

Use the `group` option in the mapping function to visualize an epicurve with different groups. If there is more than one grouping factor, use the `facet_wrap()` option, as demonstrated in the example below:

```{r}
# Plot daily incidence by sex with facets
ggplot2::ggplot(data = daily_incidence_2) +
  geom_histogram(
    mapping = aes(
      x = as.Date(date_index),
      y = count,
      group = sex,
      fill = sex
    ),
    stat = "identity"
  ) +
  theme_minimal() + # apply minimal theme
  theme(
    plot.title = element_text(face = "bold",
                              hjust = 0.5), # bold and center the title
    plot.subtitle = element_text(hjust = 0.5), # center the subtitle
    plot.caption = element_text(face = "italic", hjust = 0), # italic caption
    axis.title = element_text(face = "bold"), # bold axis labels
    axis.text.x = element_text(angle = 45,
                               vjust = 0.5) # rotate x-axis text for readability
  ) +
  labs(
    x = "Date", # x-axis label
    y = "Number of cases", # y-axis label
    title = "Daily Outbreak Cases by Sex", # plot title
    subtitle = "Incidence of Cases Grouped by Sex", # plot subtitle
    caption = "Data Source: Simulated Data" # caption for additional context
  ) +
  facet_wrap(~sex) + # create separate panels by sex
  scale_x_date(
    breaks = breaks, # set custom date breaks
    labels = scales::label_date_short() # short date format for x-axis labels
  ) +
  scale_fill_manual(values = c("lightblue",
                               "lightpink")) # custom fill colors for sex
```

# Early Analysis Calculations

Exploraty analysis and visualizations are useful, but ultimately may not be sufficiently quantitative for public health operational needs. Understanding trends in surveillance data is crucial for understanding epidemic drivers and dynamics. This may include forecasting disease burden, planning future public health interventions, and assessing the effectiveness of past control measures. By analyzing trends, policymakers and public health experts can make informed decisions to mitigate the spread of diseases and protect public health.

## Simple Model

Aggregated case data over specific time units (i.e. incidence data), typically represent the number of cases that occur within that time frame. We can think of these cases as noisy observations generated by the underlying epidemic process (which we cannot directly observe). To account for randomness in the observations, we can assume that observed cases follow either `Poisson distribution` (if cases are reported at a constant average rate over time) or a `negative binomial (NB) distribution` (if there is potential variability in reporting over time). When analyzing such data, one common approach is to examine the trend over time by computing the rate of change, which can indicate whether there is exponential growth or decay in the number of cases. Exponential growth implies that the number of cases is increasing at an accelerating rate over time, while exponential decay suggests that the number of cases is decreasing at a decelerating rate.

The `i2extras` package provides methods for modelling the trend in case data, calculating moving averages, and exponential growth or decay rate.

```{r, warning=FALSE, message=FALSE}
# load packages which provides methods for modeling
library(i2extras)
library(incidence2)

# read data from {outbreaks} package
covid19_eng_case_data <- outbreaks::covid19_england_nhscalls_2020

# subset the covid19_eng_case_data to include only the first 3 months of data
df <- subset(
  covid19_eng_case_data,
  date <= min(date) + 90
)

# uses the incidence function from the incidence2 package to compute the
# incidence data
df_incid <- incidence2::incidence(
  df,
  date_index = "date",
  groups = "sex"
)

# fit a curve to the incidence data. The model chosen is the negative binomial
# distribution with a significance level (alpha) of 0.05.
fitted_curve_nb <-
  i2extras::fit_curve(
    df_incid,
    model = "negbin",
    alpha = 0.05
  )

# plot fitted curve
plot(fitted_curve_nb, angle = 45) +
  ggplot2::labs(x = "Date", y = "Cases")
```

## Exponential Growth or Decay Rate

The exponential growth or decay rate, denoted as $r$, serves as an indicator for the trend in cases, indicating whether they are increasing (growth) or decreasing (decay) on an exponential scale. This rate is computed using the so-called **renewal equation** [(Wallinga et al. 2006)](https://royalsocietypublishing.org/doi/10.1098/rspb.2006.3754), which mechanistically links the reproductive number $R$ of new cases (i.e. the average number of people that a typical case infects) to the generation interval of the disease (i.e. the average delay from one infection to the next in a chain of transmission). This computational method is implemented in the `{i2extras}` package.

Below is a code snippet demonstrating how to extract the growth/decay rate from the above **negative binomial**-fitted curve using the `growth_rate()` function:

```{r, message=FALSE, warning=FALSE}
rates_nb <- i2extras::growth_rate(fitted_curve_nb)
rates_nb <- base::as.data.frame(rates_nb) |>
  subset(select = c(sex, r, r_lower, r_upper))
base::print(rates_nb)
```

## Peak Time

The **peak time** is the time at which the highest number of cases is observed in the aggregated data. It can be estimated using the `i2extras::estimate_peak()` function as shown in the below code chunk, which identify peak time from the `incidence2` object `df_incid`.

```{r, message=FALSE, warning=FALSE}
peaks_nb <- i2extras::estimate_peak(df_incid, progress = FALSE) |>
  subset(select = -c(count_variable, bootstrap_peaks))

base::print(peaks_nb)
```

## Moving Average

A moving or rolling average calculates the average number of cases within a specified time period. This can be achieved by utilizing the `add_rolling_average()` function from the `{i2extras}` package on an `incidence2 object`. The following code chunk demonstrates the computation of the weekly average number of cases from the `incidence2` object `df_incid`, followed by visualization.

```{r, warning=FALSE, message=FALSE}
library(ggplot2)

moving_Avg_week <- i2extras::add_rolling_average(df_incid, n = 7L)

plot(moving_Avg_week, border_colour = "white", angle = 45) +
  ggplot2::geom_line(
    ggplot2::aes(
      x = date_index,
      y = rolling_average,
      color = "red"
    )
  ) +
  ggplot2::labs(x = "Date", y = "Cases")
```

This can also be done on longer windows:

```{r, warning=FALSE, message=FALSE}
moving_Avg_mont <- i2extras::add_rolling_average(df_incid, n = 30L)

base::plot(
  moving_Avg_mont,
  border_colour = "white",
  angle = 45
) +
  ggplot2::geom_line(
    ggplot2::aes(
      x = date_index,
      y = rolling_average,
      color = "red"
    )
  ) +
  ggplot2::labs(x = "Date", y = "Cases")
```
